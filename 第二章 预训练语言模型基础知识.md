# 第二章 预训练语言模型基础知识

## 什么是语言模型？
简单来讲，语言模型就是计算一个句子的概率。
(给定一个句子，根据前文计算下一个词的概率。)

### 1. 统计语言模型
其基本思想是计算条件概率
由于用链式法则计算需要列举下一个词和其他词共同出现的所有可能的情况，并统计其在所有文档中出现的次数，然而这个词可能基本不可能与前文同时出现，这会导致最后得到的组合概率会有很多0的存在。
#### 更简单的方法来计算条件概率：**马尔科夫链**
我们可以假设预测的下一个词只和其前一个词有相关性，如果一个词的相关性不明显，则可以讲条件放宽至两个词。
当k = 1时，整个公式简化为一个单元语言模型，这样只需计算每个词在文本中出现的概率即可。
由于单元语言模型不依赖上下文的词，难以抓住句子的语义特征，因此在实际中效果通常较差。在工程实践中通常将k设为2、3...n，成为n元语言模型。

#### 困惑度
作为一种量化指标，困惑度通常用于判断一个句子的好坏。
句子的概率值越大，其困惑度PPL(W)越小，语言模型也就越好。

#### 平滑
在实际计算过程中由于数值稀疏，为了避免出现概率值为0的情况，通常会使用平滑的策略，最简单的平滑方法是将分子和分母都加入一个非零正数。

### 2. 神经网络语言模型
神经网络语言模型引入神经网络架构来估计单词的分布。
可以同时学习单词的分布式表示和单词序列(句子)的概率，而单词之间的相似度则通过词向量的距离来衡量（计算词向量的余弦相似度），因此未登录单词序列的概率也可以通过相似词进行估计，从而避免了数据稀疏问题。
**主要思想**
训练数据是一个单词序列，单词序列包含在字典中，其中字典V是一个有限集合。
模型的目标是学习一个语言模型，通过n-1个单词预测下一个单词。
整个神经网络语言模型是一个三层的神经网络。
1. 第一层是输入层，输入n-1个单词，通过查找表可以获得每个单词对应的词向量并存放在一个大小为|V| ✖️m的矩阵C中，m代表词向量的维度，直观上理解就是矩阵的每一行代表一个单词。
2. 第二层是隐藏层，作用是把输入数据的特征抽象到另一个维度空间，来展现其更抽象化的特征。
3. 第三层是输出层，一共有|V|个输出节点，每个节点是词典中每个单词的未归一化的log概率值。
模型的大多数运算成本都集中在矩阵乘法中。整个模型的架构非常简单，在训练过程中，通过随机梯度下降即可得到最终结果，由于自带平滑效果无需平滑算法。
整个模型训练完毕，除了语言模型还可以直接获取“中间产物“——单词的分布式表示（词向量）。

### 3. 词向量：解决相似单词的距离问题
将单词用向量的方式表示，是将深度神经网络引入自然语言处理领域的一个核心技术。


在自然语言处理任务中，训练数据集的最小单元通常是一个字或词，那么该如何将它们表示为一个特征序列？
所谓“表示”是将词转换为计算机适合理解和处理的格式，而计算机天然适合处理数值类数据，因此词向量作为一种表示方式，让语言变成计算机可以处理的数值。
#### 背景：
早期的词表示方式是独热表示，但存在维度过高和无法解决词之间的相似问题。
词的分布式表示：对于无法解决词之间的相似问题，一个可行的方法是将词表示成维度较低的特征序列，例如人为地设计不同的特征，并给每个词分配不同值，再去计算词之间的相似度。
那么能否让算法自动地学习不同维度的特征呢？下面以word2vec模型为例进行分析

#### word2vec模型结构
word2vec模型的输入是独热向量，那么最终的低维词向量是如何通过训练的得到的呢？
对于一个维度为10000的字典，则每个单词的独热表示维度也是10000，如果希望得到词向量维度为300，则隐层的权重矩阵设为10000✖️300，每一个单词的独热表示乘以隐层的权重矩阵，即可得到对应的3维词向量。训练结束后，矩阵的每一行代表着一个词的300维词向量。

#### 如何降低运算成本
word2vec采用了采用了两种计算方法，分别是层次Softmax和负采样，大大降低了运算成本。
**层次Softmax**：由于输出层最后预测的是一个多分类问题，类别数量为字典大小，通过层次Softmax构造的哈夫曼树可以讲V分类的问题变成log(|V|)次的二分类问题。
**负采样**：在训练过程中可以每次只调整部分网络参数，而不是全量参数，减少资源损耗。负采样方法本质上是对训练集进行了采样，从而减少了训练集的大小。

#### 缺点
由于这些词表示方法本质上是静态的，每一个词都有一个唯一确定的词向量，不能根据句子的不同而改变，因而无法处理多义词问题。
对于多义词问题ELMo提出了较好的解决方案，其核心思想是仅训练一个语言模型，在以不同的句子作为输入时，模型根据上下文来推断每个词代表的词向量。
ELMo在训练过程中使用了双向长短期记忆网络（LSTM)，在了解ELMo之前先了解一下循环神经网络和LSTM的基础。

### 4. RNN和LSTM基础
传统的神经网络无法获得时序信息，RNN的出现使处理时序信息成为可能。
#### RNN：
在某个时刻输入一个变量，通过RNN的一个基础模块输出一个值，而刚刚那个时刻的信息，可以通过一个循环传递给下一个时刻，子子孙孙无穷尽也。
RNN可以看做多个基础模块互连，每一个模块都将信息传递给下一个模块。
**缺点**：虽然可以解决时序问题，但是这里的时序一般是短距离的，对于相关信息距离较远的情况，RNN很难学习到这些信息。
LSTM的提出很好地解决了长距离依赖的问题。
#### LSTM：
LSTM中包含三个门，分别是遗忘门、输入门、输出门。
1. 遗忘门接收上一个时刻的状态和当前的输入，经过Sigmoid函数输出一个0到1之间的值用于判断保留多少信息(1则全保留，0则全丢弃)。
2. 输入门决定哪些新信息被留下来，并更新细胞状态。输入门的取值由上一个时刻的状态和当前输入决定，同样通过Sigmoid函数得到一个0到1的值，而tanh函数创造了一个当前细胞状态的候选。直观上的理解是，通过遗忘门得出遗忘的程度，再将其与旧细胞状态相乘，“忘记”之前状态的一些信息，再加上输入门得出的更新程度乘以新细胞状态的候选，增加新状态的一些信息。
3. 输出门决定了最后的输出信息。输出门的取值同样也是由上一个时刻的状态和当前输入决定的，通过Sigmoid函数得到一个0到1的值，最后通过tanh函数决定输出。

### 5. 基于RNN的语言模型
**如何体现时序：**
在某个时刻输入新的单词后，会和上一个时刻的隐层状态计算出下一个隐层状态。每个隐层都通过一个前馈神经网络得到输出值。
为了降低训练成本，在后向传播中使用了批处理模式，即处理完多个训练样本后才更新一次权重。
**优势：**
可以最大化利用前面所有词的信息，来判断下一个词的概率，而早期的语言模型只能利用前面固定的n个词来预测。
**问题：**
1. 训练成本较高
2. 判断单词概率时，无法捕捉后续句子信息对单词的影响。双向LSTM的结构很好地解决了此类问题。

### 6.ELMo：解决多义词的表示问题
ELMo本质上是一个双向的LSTM结构的语言模型。
对于一个单词序列，前向LSTM语言模型通过某个单词单词的“上文”（该单词前面的所有单词）计算概率，后向LSTM语言模型则通过单词的“下文”（该单词后面的所有单词）计算当前单词的概率。而双向语言模型整合了某个词的“上文”和“下文”的信息，最大化了以下对数似然概率：











