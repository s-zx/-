# 第一章 预训练语言模型简介

## 绝大部分的自然语言处理任务可以归为五类：

1. 分类任务
给字符串添加类标，应用于文本分类和情感分类
2. 匹配任务
匹配两个字符串，应用于搜索、问答
3. 翻译任务
将一个字符串转化为另一个字符串，应用于机器翻译和自动语音识别
4. 结构化预测任务
将字符串映射到特定结构，应用于命名实体识别、分词、词性标注、语义分析
5. 序列决策处理任务
在动态变化的环境中采取状态操作，应用于多轮对话。

## 困难：
在监督学习的场景下，每一类nlp任务都需要特定的模型并且需要大量的标注数据，不同于图片标注，nlp标注任务难度通常较高，需要耗费大量人力物力。
因此缺乏大规模标注数据是nlp研究领域的一大难题。

## 不用标注数据行不行？
相较于标注数据，大规模无标注文本数据的获取则容易得多，如果能利用这些数据就能大幅提升自然语言处理任务的效果。
而预训练语言模型就是通过对这些无标签的数据进行“预训练”，获得一个较好的语言表示，再将其应用到特定的nlp下游任务中。
**举个例子**，“预训练”的过程可以理解为“修炼内功”的过程，党内功修炼好了之后再学习其他武功招式就如鱼得水，进步神速了！“预训练”也可以比喻为大树的根部，每一片树叶都是一个下游任务。

## 预训练语言模型的训练方法
先在大规模文本中训练出通用的语言表示，再通过**微调**进行下游任务的领域适应。

## 为什么要预训练？

### 预训练模型
预训练属于迁移学习的范畴。现有的神经网络在训练时，先对网络的参数进行随机初始化，再通过优化算法优化模型参数，而预训练的思想时先通过一些任务进行预先训练，得到一套模型参数，在此基础上对模型进行初始化，再进行训练。

对于计算机视觉领域的预训练过程，将图片转换为计算机可处理的表示形式（如RGB），就可以输入之神经网络进行后续处理，而对自然语言来说，如何进行**表示**时首先要考虑的问题。

### 自然语言表示
以word2vec为代表的第一代预训练语言模型中，一个单词的词向量是固定不变的，因此对单词进行向量表示的过程中，不会区分单词的不同含义，这就导致无法区分多义词的不同语义。
EMLo考虑了上下文的词向量表示方法，以双向LSTM作为特征提取器，同时考虑了上下文的信息，从而较好地解决了多义词的表示问题，开启了第二代预训练语言模型的时代，即“预训练+微调”的范式。

### 自回归模型和自编码模型
自回归模型是根据上文预测下一个单词，或根据下文预测上一个单词。ELMo将两个方向的自回归模型进行拼接，实现双向语言模型，本质上仍属于自回归模型。
自编码模型（如BERT），通常被称为降噪自编码模型，在输入中随机掩盖一个单词（加入噪声），在预训练时根据上下文预测被掩码词（降噪）。
这个模型的好处是可以同时利用被预测单词的上下文信息，劣势时在下游的微调阶段不会出现掩码词，因此[MASK]标记会导致预训练和微调阶段不一致的问题。
通过对bert的观察发现，对于预训练语言模型，随着模型由浅入深，特征会也更具体。

### 预训练语言模型的发展
目前，预训练语言模型的通用范式是：
1. 基于大规模文本，预训练得出通用的语言表示。
2. 通过微调的方式，将学习到的知识传递到不同的下游任务中。

### 对主流预训练语言模型的分类标准
1. 语言表示是否与上下文相关。
2. 模型的核心结构
3. 任务类型
4. 模型扩展



