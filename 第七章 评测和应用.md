# 第七章 评测和应用

  ### 通用评测任务

GLUE（General Language Understanding Evaluation）任务发布于2018年，集成了多个子任务，任务数据规范，自发布以来所有的预训练语言模型都以在GLUE上实现SOTA效果为目标。

GLUE共包含9个自然语言理解任务，同时还包含一个小型的诊断数据集，由人工构建，用于评测系统性能。诊断数据集预先定义了一些比较有价值的分类，可以很好地捕捉模型在特定分类下的表现。

在GLUE发布一年后，SuperGLUE作为一个新的评测任务被提出，它是继GLUE之后的新的基准测试，任务更具挑战性，任务资源也进行了重新整合。

## 模型压缩：量化、剪枝和蒸馏

### BERT模型分析

BERT的结构根据具体的实现逻辑，可以分为Embedding层、Linear before Attention层、Multi-Head Attention层、Linear after Attention层和Feed Forward层，后4层属于Transformer Block内的模块，所需存储空间和推理耗时会随着层数的增多而增多。

BERT的大小可以用3个超参数来衡量，即L、H和A，其中L表示Transformer Block的层数，H表示隐层向量的头数。通过这三个超参数，可以基本知晓BERT的各模块大小，L和H决定了模型的宽度和深度，A决定了模型Attention的多样性。

### 量化

模型量化是指将模型权重参数用更少的比特数存储，以此来减少模型的存储空间和算力消耗。

量化是一种通用的压缩方法，使用于几乎所有的深度模型。学术界已经证实，全连接层是对量化操作十分友好的，而BERT的大部分模块都是由全连接层组成，因此BERT对于量化操作十分友好。

> 值得注意的是，Embedding层对于量化操作较为敏感，所以在实际操作中应避免对Embedding层做量化操作

数据精度降低之后，模型的质量肯定会下降，一般可以通过量化后训练(Quantization Aware Training)来缓解。具体而言，在对模型参数进行直接量化操作或精度截断之后，用训练数据继续训练量化后的模型，以缓解量化造成的精度损失，这是常见的量化压缩流程。

### 剪枝

模型剪枝是指去除模型参数中冗余或不重要的部分，剪枝是提高推断效率的方法之一，可以高效地生成规模更小、内存利用率更高、能耗更低、推断速度更快的模型。就BERT的剪枝而言，可以大致分为两类：元素剪枝和结构剪枝。

元素剪枝又称为稀疏剪枝，聚焦于模型单个参数元素。若单个参数元素绝对值过小或对模型不重要，则可以通过将其置0来减小存储空间，缩短推理时间。对模型不重要的定义可以是对目标函数影响小，也可以是对梯度更新影响小等自定义的客观衡量标准。元素剪枝对全连接层相对友好，因此BERT使用元素剪枝可以获得不俗的压缩比，同时还可以保证一定的精度。

结构剪枝聚焦于取出模型结构的冗余，以精简模型结构来减小模型的存储空间，满足算力需求。结构剪枝更具有针对性，对于不同的模型结构，结构剪枝可以设计不同的剪枝策略。以BERT为例，一般由两种结构剪枝策略：Attention头剪枝和层剪枝。

1. Attention头剪枝：BERT的Multi-Head Attention层在推理时间中占比排第二，研究表明，Multi-Head Attention层存在较大的冗余，因此BERT的12-Head Attention可以通过剪枝变为4-Head或更少，这样可以大大缩短Multi-Head Attention层的推理时间。
2. 层剪枝：BERT是由多层Transformer Block堆叠而成的，层数越多，模型所需存储空间越大，推理时间也越长。训练时常用的Dropout操作有助于模型收敛得更稳定，故通过删除个别不重要的层来减少模型参数、加速推理是完全可行的。不重要的层可以通过比较目标函数值大小和L1正则化值的大小方法来定位。

剪枝操作也会对模型带来精度损失。一般而言，根据剪枝流程的位置可以将剪枝分为训练时剪枝和后剪枝。

训练时剪枝和训练时使用的Dropout操作较为类似，训练时剪枝会根据当前模型的结果，删除不重要的结构，固化模型再进行训练，以后续的训练来弥补部分结构剪枝带来的不良影响。

后剪枝则是在模型训练完成后，根据模型权重参数和剪枝测试选取需要剪枝的部分，比较简单粗暴，但是与训练时剪枝所需的额外计算量和控制流程相比，后剪枝是较为简单的做法。









